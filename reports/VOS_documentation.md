# 딥러닝 기반 동영상 객체 분할 기술 동향 요약

## 1. 동영상 객체 분할(VOS) 기술 개요
동영상 프레임 내의 특정 객체 영역을 배경으로부터 분할해 내는 기술입니다사용자의 개입 정도에 따라 크게 **비지도(Unsupervised)**, **준지도(Semi-supervised)**, **인터렉티브(Interactive)** 세 가지로 분류됩니다.

## 2. 비지도 동영상 객체 분할
사용자가 어떠한 정보도 주지 않은 상태에서 배경으로부터 객체를 분할해 내는 방식입니다. 주로 동영상에서 자주 출현하는 '주요 객체'를 자동으로 탐지하는 것에 초점을 맞추고 있습니다.

* **기술 동향:** RGB 영상과 움직임 정보를 함께 처리하는 인코더-디코더 구조의 심층 신경망이 주로 사용됩니다. 정지해 있는 객체에 대한 정확도를 높이기 위해 특징 맵의 벡터 간 내적을 통해 객체를 찾는 **프레임 간 정합(matching)** 기법이 대세를 이루고 있습니다.

### 벡터 간 내적을 사용하는 이유
세 개의 요소를 가진 두 특징 벡터 a = [a1, a2, a3]와 b = [b1, b2, b3]가 있을 때, 대수적 관점의 내적 연산은 다음과 같이 진행됩니다.

a · b = a1*b1 + a2*b2 + a3*b3

기하학적 관점에서 이 계산은 두 벡터의 크기와 사이각 θ(세타)를 이용해 아래와 같은 형식으로도 계산할 수 있습니다.

a · b = |a| * |b| * cos(θ)

이 코사인(cos) 함수의 성질로 인해 두 특징 벡터의 각도에 따라 세 가지 결과로 나뉩니다.
* **같은 방향 (0°):** cos(0°) = 1이 되어 내적 값이 최대가 됩니다. 이는 두 특징이 매우 유사하다는 것을 의미합니다.
* **직각 (90°):** cos(90°) = 0이 되어 내적 값이 0이 됩니다. 이는 두 특징 간의 연관성이 없다는 것을 의미합니다.
* **정반대 방향 (180°):** cos(180°) = -1이 되어 내적 값이 최소가 됩니다. 이는 두 특징의 성향이 정반대임을 의미합니다.

### 투 스트림(Two-stream) 방식
서로의 단점을 완벽하게 보완하기 위해 두 갈래의 스트림 경로를 이용합니다. 예를 들어 숲속에 보호색을 띤 동물이 움직이는 순간에는 광흐름 스트림이 이를 포착하고, 객체가 갑자기 움직임을 멈추어 광흐름 정보가 0에 가까워져도 RGB 스트림이 뚜렷한 외형 정보를 유지해 객체를 놓치지 않습니다.

* **스트림 1 (RGB 색상 영상):** 일반적인 동영상 프레임 이미지로, 객체의 색상, 질감, 형태, 윤곽선 같은 시각적(공간적) 특징을 꼼꼼하게 추출합니다.
* **스트림 2 (광흐름 크기 영상):** 인접한 두 프레임을 비교하여 픽셀들의 이동 방향을 계산한 데이터로, 대상의 독자적인 궤적이나 움직임 같은 시간적 특징 추출에 집중합니다.

## 3. 준지도 동영상 객체 분할
동영상의 첫 프레임에서 사용자가 타겟 객체에 대한 정확한 분할 영역을 제공하면, 이를 이용해 이후 프레임들에서 타겟 객체를 분할합니다.

* **기술 동향:** 초기에는 첫 프레임 정보를 이용해 신경망을 미세 조정(fine-tuning)하는 방식을 시도했지만, 속도와 객체 변형 대처에 한계가 있었습니다. 이를 극복하기 위해 분할 영역 전파 및 보정 방식이 제안되었으며, 최근에는 임베딩 공간에서 타겟 객체와 특징 거리가 가까운 픽셀들을 분류해 내는 정합 방식이 속도와 정확성 모두 우수한 결과를 보여주고 있습니다.
- 샴 네트워크: 똑같이 생긴 두 개의 신경망(인코더)을 나란히 이어 붙인 구조를 의미한다.
    - 특징: 두 개의 서로 다른 입력 데이터를 각각 똑같은 구조와 가중치를 가진 두 개의 네트워크에 동시에 통과시켜서 서로의 특징을 비교하는데 특화되어있다.
    - 이전 프레임의 결과를 현재 프레임으로 넘겨주는 전파 방식이 움직임 정보에 지나치게 의존하는 단점을 극복하기 위해 제안되었다.
    - 작동원리
    1. 입력 데이터 분리
        1. 첫 번째 인코더: 사용자가 정확히 짚어준 ‘첫 프레임 영상’과 ‘타겟 객체 마스크’를 입력받아. 신경망에게 “우리가 쫓아가야 할 목표물은 정확히 이렇게 생겼어”라고 확실한 기준점을 알려주는 과정이다.
        2. 두 번째 인코더: 목표물을 찾아내야 할 ‘현재 프레임 영상’만을 입력으로 받는다.
    2. 특징 및 추출 연결: 쌍둥이 인코더는 각각의 입력 데이터로부터 복잡하고 깊은 특징(Feature) 정보들을 추출해낸다. 그 뒤에 두 인코더에서 나온 특징 맵들을 하나의 텐서로 나란히 이어붙인다.
    3. 최종 분할 영역 예측: 이렇게 연결된 정보들을 디코더에 통과시키면, 현재 프레임 안에서 타겟 객체에 해당하는 부분만 정확히 분할해 낼 수 있다.

### 4. 임베딩 공간(Embedding Space) 기반 정합 기술

- 임베딩 공간: 추출된 특징 벡터들이 존재하는 추상적인 수학적 차원이다. 이 공간의 핵심은 비슷한 특징을 가진 데이터는 서로 가까운 위치에 모이고, 성질이 다른 데이터는 멀리 떨어진다는 점이다. 이 원리를 이용하면 타겟 객체와 유사한 특징을 갖는 픽셀들을 분할 결과로 도출하는 기법도 사용할 수 있다.
    - PML(Pixel-Wise Metric Learning): 인코더를 통해 첫 프레임과 현재 프레임의 특징 맵을 각각 추출하여 임베딩 공간 안에서 현재 프레임의 픽셀들이 첫 프레임의 ‘타겟 객체’ 영역과 거리가 가까운지, 아니면 ‘배경’ 영역과 거리가 가까운지 특징 거리를 측정한다. 그리고 거리가 가장 가까운 쪽의 속성을 따라가는 최근접 이웃 분류(nearest neighbor classifier)를 통해 해당 픽셀이 객체인지 배경인지  결정하는 방식이다. 정확한 거리 측정이 가능하도록 임베딩 공간 자체를 잘 구성하도록 학습시켜 예측 정확도를 높였다.
    - FEELVOS: 첫 프레임과 현재 프레임의 거리를 측정하는 전역적 정합(global matching) 방식에 더해서, 바로 맞닿아 있는 인접 프레임 간의 지역적 정보까지 고려하는 지역적 정합(local matching)을 추가로 수행한다. 전체적인 기준점(첫 프레임)과 바로 직전의 흐름(인접 프레임)을 모두 고려하니 기존보다 더 우수한 결과를 보여줬다.
    - STM(Space-Time memory): 2020년 4월 기준 가장 빠른 성능과 속도를 자랑하는 모델이다. 앞의 모델들이 두세 개의 프레임 사이에서만 정합을 수행했던 반면에, 이 방식은 비지역적(non-local) 신경망 기술을 사용했다. 즉, 동영상 내 여러 프레임들의 정합 반응 값을 한번에 효율적으로 활용하도록 심층신경망을 구성했다. 과거의 다양한 프레임 정보들을 효율적으로 기억해두고 현재와 비교하는 방식이다.
- 위의 세 모델들은 결국 복잡하게 얽혀있는 동영상 속에서도, 임베딩 공간이라는 똑똑한 잣대를 통해 ‘내가 찾고자 하는 객체의 특징’과 가장 일치하는 픽셀들만 정교하게 찾아내는 기술을 핵심적으로 사용하고 있다.

## (추가 공부)5. 2026년 기준 최신 동영상 객체 분할(VOS) 기술 동향 업데이트

해당 논문이 작성된 2020년 4월은 딥러닝 기반 VOS 기술의 과도기였으며, 2026년 현재 컴퓨터 비전 분야의 객체 분할 패러다임은 다음과 같이 눈부시게 발전했다.

### 5-1. CNN에서 Transformer로의 대전환
* **과거:** ResNet 등 CNN(합성곱 신경망) 기반의 인코더-디코더 구조가 주로 사용되었다.
* **현재:** 자연어 처리(NLP)를 휩쓸었던 트랜스포머(Transformer) 구조가 비전 분야의 표준으로 자리 잡았다. 현재 SOTA(State-of-the-Art) 모델들은 대부분 **ViT(Vision Transformer)**나 **Swin Transformer**를 백본(Backbone)으로 사용하며, CNN보다 전역적인(Global) 문맥 정보를 훨씬 잘 파악하여 객체가 가려지거나 형태가 크게 변해도 안정적으로 추적할 수 있다.

### 5-2. 파운데이션 모델(Foundation Model)의 등장
* **과거:** 특정 데이터셋(DAVIS, YouTube-VOS 등)으로 모델을 목적에 맞게 지도 학습(Supervised Learning)시켜야만 했다.
* **현재:** Meta에서 공개한 **SAM(Segment Anything Model)** 시리즈가 생태계를 재편했다.
  * **SAM 2 (2024년 발표):** 별도의 추가 학습 없이(Zero-shot) 이미지와 비디오 내의 모든 객체를 엄청난 정밀도로 자동 분할해낼 수 있다. 과거 논문에서 다루던 복잡한 프레임 간 정합 연산들이 거대한 파운데이션 모델 내부에서 훨씬 정교하게 통합 처리되고 있다.

### 5-3. 메모리 네트워크의 효율성 극대화
* **과거:** 논문에서 언급된 최고 성능의 STM(Space-Time Memory)은 뛰어난 매칭 성능을 보였으나, 영상이 길어지면 메모리 과부하가 발생하는 한계가 있었다.
* **현재:** 어텐션(Attention) 메커니즘을 효율화한 **AOT(Associative Object Tracking)** 기술이나, 장기(Long-term)와 단기(Short-term) 메모리를 분리하여 관리하는 **XMem** 아키텍처가 도입되어, 아주 긴 영상에서도 메모리 부족 없이 다중 객체를 동시에 추적할 수 있게 되었다.

### 5-4. 멀티모달 프롬프트 기반 분할 (Referring VOS)
* **과거:** 준지도(Semi-supervised) 방식을 위해 사용자가 직접 첫 프레임에 마스크(정답 픽셀 영역)를 칠해 주어야 했다.
* **현재:** 사용자가 텍스트 프롬프트로 명령을 내리면 모델이 알아서 객체를 찾는 **RVOS(Referring Video Object Segmentation)** 기술이 상용화되었다. (예: "왼쪽에서 달려오는 빨간 차 분할해 줘"라고 입력하면 AI가 알아서 해당 객체를 탐지하고 영상 내내 추적함)

---
### 요약: 2020년 vs 2026년 VOS 패러다임 비교

| 구분 | 2020년 (논문 작성 시점) | 2026년 (최신 트렌드) |
| :--- | :--- | :--- |
| **핵심 아키텍처** | CNN (ResNet, 멀티 스트림 등) | **Transformer (ViT, SAM 2)** |
| **메모리 기법** | STM (단순 프레임 누적) | **XMem (장/단기 분리 메모리), AOT** |
| **학습 패러다임** | 특정 데이터셋 기반 지도 학습 | **대규모 파운데이션 모델 (Zero-shot)** |
| **사용자 입력 방식** | 마스크(Mask) 제공 필수 | 점(Point), 박스(Box), **텍스트(Text) 프롬프트** |

---
**참고문헌**
* 고영준, "딥러닝 기반 동영상 객체 분할 기술 동향", 방송과 미디어 제25권 2호, 2020.
